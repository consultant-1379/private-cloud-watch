#!/usr/bin/python
import re
import datetime
import math
import fileinput


# find Log lines generated by the High Availability (HA) Workflow system.
# and track id's in a dictionary
class HAidFinder:

    # Use regex for HA_0-9* then pull off HA prefix to get bare id
    prefix = "HA_"
    exampleID = "1555575545764"
    idlen = len(exampleID)

    def __init__(self):
        # All matches aren't HA id's, such as: HA_tbaytelenm01-opendj-0
        self.p = re.compile('HA_[0-9a-zA-Z_-]*')
        self.lcnt = 0
        self.goodIDs = 0
        self.idList = {}

    # Call after all lines processed to print summary of findings.
    def Report(self):
        print "HA event report...  %d Good ID's  in %d lines" % (self.goodIDs, self.lcnt)
        print self.idList


    # process a line from a log file.
    def IngestLine(self,line, log_time):
        self.lcnt += 1
        finds = self.p.findall(line)
        if len(finds) == 0:
            return

        badID = False
        if len(finds) > 1:
            print ("Warning. More than one HA_ found. Using first one: ",finds)
        found = finds[0]
        flen = len(found)
        # ID's can have different prefixes, but the N digit id is always (as of July 2019) at the end.
        id = found[flen-self.idlen:]

        # Reject the cases where HA_ exists, but it's not really an ID.
        try:
            int(id)
        except ValueError:
            badID = True

        if len(id) < self.idlen:
            badID = True

        if badID != True:
            if id in self.idList:
                self.idList[id] += 1
            else:
                self.goodIDs += 1
                self.idList[id] = 1

        return

# Extract timestamp from start of ENM log line
def logTime(line):

    LOG_TIME_FORMAT = "%Y-%m-%d %H:%M:%S,%f"
    ExampleTime = "2019-04-18 04:20:56,513"
    # A small number of lines come in with this format.
    # Ignoring for now as they don't seem to have much signal:
    # INFO [o.n.k.i.s.f.RecordFormatSelector] Format not configured. Selected format from the store: RecordFormat:StandardV3_4[v0.A.9]
    # OtherFormat = "2019-07-21 15:47:43.358-0400"  #period, not comma

    LogTimeLen = len(ExampleTime)
    LogTime = line[0:LogTimeLen]

    #TODO: Some logs have  different format:  Aug 19 05:48:47

    try:

        dt = datetime.datetime.strptime(LogTime, LOG_TIME_FORMAT)
        # TODO: Adjust time to a common time zone. Must pass in correction factor "hours"
        #nt = AdjustTime(dt, hours)
        return dt
    except ValueError:
        # We could try harder to determine if it's a valid timestamp,
        # but in a different format and return a different exception.
        # But the HA logs are all using the time format specified here
        # for now.
        raise ValueError("No date or invalid date format")


# Return a "bucket time" for a given datetime.
# Used to bucketize times for time histograms
def bucket(datetm, bucket_size_minutes):
    # The bucket size must evenly divide into an hour or buckets
    # intervals won't be constant.
    minute_boundary = ( datetm.minute/bucket_size_minutes ) * 30
    bkt_time = datetm.replace(minute=minute_boundary,second=0,microsecond=0)

    return bkt_time


# A Class/Struct to store events and their associated data for later
# processing and display. Most likely key'ing on VM Name or HA ID, and
# then grouping on the other one.  It's starting to smell a lot like
# SQL.....
# Currently the Events are occurrences of DELETE_COMPLETE in a log file
class EventInfo():
    def __init__(self):
        self.firsttime = None  # First time event  was encountered
        self.lasttime = None   # Last time encountered (DELETE)
        self.total_event_count = 0         # Number of times encountered in log.
        self.EventTimes = []   # Could be per-group, but we don't need that granularity yet.
        self.Groups = {}       # Key is usually VM name, or HA id. with grouping on the other won

    def Timespan(self):
        span = self.lasttime - self.firsttime
        # Remove microseconds, since the variable length messes up formatting (and we don't need it)
        span = datetime.timedelta(seconds=math.ceil(span.total_seconds()))
        return span

    def NumberGroups(self):
        return len(self.Groups)

    def TopGroup(self):
        # sort by occurence, reverse, pick biggest
        # or just scan through looking for biggest, not full sort
        biggest = 0
        group = "none"
        for (key,val) in self.Groups.items():
            if val > biggest:
                biggest = val
                group = key
        return group, biggest

    def GetEventTimes(self):
        self.EventTimes.sort()
        return self.EventTimes


    # Instead of showing ALL occurrences, try to show only the Top N
    # ones that are "close" together.  Trying to highlight problem
    # periods vs drowning the user in noise.
    def GetEventNeighbors(self, N):
        self.EventTimes.sort()

        deltas = []
        neighbors = []  # Store the datestamps (pairs) for smallest N deltas
        prev = 0
        found = 0
        for t in self.EventTimes:
            if prev != 0 :
                deltas.append( t - prev )
                #print ">>>>>>>>>> DELTA ",(t-prev)
            prev = t

        deltas.sort()

        for d1 in deltas:
            prev = 0
            for t in self.EventTimes:
                if prev != 0 :
                    d2 = t - prev
                    # No Exit in this block since we want to find ALL
                    # deltas of this duration, not just the first.
                    if d2 == d1:
                        neighbors.append((prev,t))
                        found += 1
                        if found == N:
                            return sorted(neighbors, key=(lambda n: n[0]) )
                prev = t


        srt = sorted(neighbors, key=(lambda n: n[0]) )
        return srt


    # A group is something like a VM name or HA id
    # The event
    def Group(self, group,time):
        self.Groups[group] = self.Groups.get(group,0)+1
        self.total_event_count += 1

        if self.firsttime is None:
            self.firsttime = time
            self.lasttime = time

        self.lasttime = max(self.lasttime, time)
        self.firsttime = min(self.firsttime,time)
        self.EventTimes.append(time)

        return



# EventCounter is a dict of Events represented by EventInfo instances
# and updated by Add(..)  Keys can be anything.  Typically tracking HA
# Workflows for a HA ID, or by VM name.
class  EventCounter():
    bucket_time_format= "%Y-%m-%d %H:%M:%S"
    def __init__(self):
        self.EInfos = {}  # AddEvent sets the keys for this dict.
        self.MaxEvents = 12  # Want column minimum width
        self.KeyWidth = 0
        self.timelen= len(datetime.datetime.now(None).strftime(self.bucket_time_format))

    # Add or update a Event.  groupby is any identifier you want to
    # sub-group the key by for later reporting.
    def AddEventKeylen(self, key, groupby, log_time, keylen):
        Event = self.EInfos.get(key)
        if Event == None:
            Event = EventInfo()
            self.EInfos[key]=Event

        # Groupby would be something like VM's for HAid's or HAid's for Vm's
        Event.Group(groupby, log_time)

        # Track for later output formatting
        self.MaxEvents=max(Event.total_event_count,self.MaxEvents)  # For screen formatting later.
        self.KeyWidth=max(keylen,self.KeyWidth)

    # Wrapper and common case where the key's len can be automatically determined
    def AddEvent(self, key, groupby, log_time):
        self.AddEventKeylen(key, groupby, log_time, len(key))

    # Wrapper If key happens to be a date or time
    def AddTimeEvent(self, key, groupby, log_time):
        self.AddEventKeylen(key, groupby, log_time,self.timelen)

    # Order the event types by number of occurrences.  Useful for histograms
    def SortedEinfos(self):
        srt = sorted(self.EInfos.items(), key=(lambda x: x[1].total_event_count) )
        srt.reverse()
        return srt


    # Assumes that EInfos key's are time or datestamps.
    def Histogram_timekey(self, bucket_minutes):
        print('\n==== Time Histogram, {} minutes per bucket'.format(bucket_minutes))
        print('====   Note: 0 occurrence buckets not displayed')

        Header = '{k:<15} {occur:-^{barwidth}} {grp:3} {top}'.format(k="key", occur="Occurrences", grp="Grps", top="TopGroup",barwidth=self.MaxEvents )
        print Header
        extra=""

        prev_bucket_time = datetime.datetime.now(None)
        bsize =  datetime.timedelta(minutes=bucket_minutes)

        # TODO: do date to string conversion for sorting, but don't alter key.
        time_ordered = sorted(self.EInfos.items(), key=(lambda x: x[0]) )
        for i in time_ordered:
            bucket_time = i[0]
            span = bucket_time - prev_bucket_time
            if span >  bsize:
                # TODO: Don't create a new line, but add as prefix to
                # current line in a fixed width field 0 i

                span = bucket_time - prev_bucket_time
                print (" "*(self.MaxEvents+26)),"Gap >> ",span

            Event = i[1]
            extra = '{:<4} {}'.format( Event.NumberGroups(), Event.TopGroup())
            #out = '{bt:<15} {bar:{width}} {end:}'.format(bt="foo", bar=('+' * Event.total_event_count), width=self.MaxEvents, end=extra )
            bucket_time_pretty = bucket_time.strftime("%Y-%m-%d %H:%M")
            out = '{bt:<15} {bar:{width}} {end:}'.format(bt=bucket_time_pretty, bar=('+' * Event.total_event_count), width=self.MaxEvents, end=extra )
            print out
            prev_bucket_time = bucket_time




    # Histogram_eventspans adds additional info about time between events:
    #  1. Datestamp of earliest occurring neighbor pair. Might not be smallest delta.
    #  2. The delta between each neighbor pair.
    # Note: deltas are in date order NOT the amount of time between the two events.
    def Histogram_eventspans(self):
        prev_time = 0
        span = 0
        Header = '{k:<30} {occur:-^{barwidth}} {first:-^19} {spn:-^19} {grp:3} {top:^12} {cnt:3} {dlt:^40}'.format(k="key", occur="Occurrences",first="First-Occur", spn="Span", grp="Grps", top="TopGroup & #Evts", cnt="",dlt="Timespan between Top N nearest events.",barwidth=self.MaxEvents )
        print Header
        for i in self.SortedEinfos():
            key = i[0]
            Event = i[1]

            want = 4  # Number of event pairs
            times = Event.GetEventNeighbors(want)
            found = len(times)  #Num times returned will be 2*wanted
            #print "\n)))))))))))))) RETURNED TIMES: ", times

            ngh = ""

            if found > 0:
                leader = None
                neighbors = []

                for t in times:
                    delta = t[1] - t[0]
                    # Print the first datestamp as semi-useful reference point.
                    if leader == None:
                        x = t[0].strftime(self.bucket_time_format)
                        x = "(First:"+x+")"
                        neighbors.append(x)
                        leader = t[0]


                    # Opting to show delta between "neighbors", but
                    # there's no hint of how far apart different
                    # neighbor pairs are. So you can't see bad periods
                    # for a VM.  Though you will hopefully see some
                    # correlation in the earlier by-time histogram
                    # showing the VMname of the top failing VM.

                    x = str(delta).split(".")[0] # Delta w/o microseconds


                    neighbors.append(x)

                ngh = '| '.join(neighbors)
                #print "\n)))))))))))))) NEIGHBORS: ",ngh
                #print ""

            TGroup,EventCount = Event.TopGroup()
            extra = '{} {:>19} {:<4} {:12} {} {}'.format(Event.firsttime.strftime("%Y-%m-%d %H:%M:%S"), Event.Timespan(), Event.NumberGroups(), TGroup, EventCount, ngh)

            out = '{k:<30} {bar:{width}} {end:}'.format(k=key, bar=('+' * Event.total_event_count), width=self.MaxEvents, end=extra )
            print out


    # Output an ascii histogram for a EventCounter or something that
    # quacks like one.
    # Histograms are ordered from most to least frequent.
    def Histogram(self, verbose):
        Header = '{k:<30} {occur:-^{barwidth}} {first:-^19} {spn:7} {grp:3} {top:}'.format(k="key", occur="Occurrences",first="First-Occur", spn="Span", grp="Grps", top="TopGroup", barwidth=self.MaxEvents )
        print Header

        # The histogram lines themselves
        for i in self.SortedEinfos():
            #self.SortedEinfos():
            key = i[0]
            Event = i[1]

            extra = ""

            if verbose:
                extra = '{} {} {:<4} {}'.format(Event.firsttime.strftime("%Y-%m-%d %H:%M:%S"), Event.Timespan(), Event.NumberGroups(), Event.TopGroup())
            else:
                extra = ""

            out = '{k:<30} {bar:{width}} {end:}'.format(k=key, bar=('+' * Event.total_event_count), width=self.MaxEvents, end=extra )
            print out



    # Simple test histogram function
    def Histogramx(self,verbose):
        for i in self.EInfos.items():
            print "{} {}".format(i[0],i[1].count)


# Compile HA workEvent occurrence counts as a way to see problems like
# VM Flapping,  Peak HA event times, Problem tenants, etc.
#  Using DELETE_COMPLETE HA log messages as a proxy for a completed HA WorkEvent
class  EventActivityFinder():
    # Field numbers, from field 0, for stuff in "DELETE_COMPLETED" lines
    del_haid_fld = 7
    del_vm_fld = 14

    def __init__(self):
        self.first_time = 0
        self.last_time = 0
        self.first_occur_time = 0
        self.last_occur_time = 0

        self.vm = EventCounter()
        self.ha_id = EventCounter() # Frequency count for HA id's
        self.timehist = EventCounter()  # TODO: get it working, remove del_times
        self.bucket_minutes = 30

        self.time_interval_minutes = 60

        # All matches aren't HA id's, such as: HA_tbaytelenm01-opendj-0
        self.re_del=re.compile('DELETE_COMPLETE')

        if 60 % self.bucket_minutes != 0:
            raise  Warning("chosen bucket size does not divide evenly into 60 minutes. Bucket durations will not be consistent.")



    # Call after all lines processed to print summary of findings.
    def Report(self):
        print "\n========================== HA Workflow REPORT ============================  "
        print "Log Span                  " , self.first_time, self.last_time
        print "Log Elapsed time          ", self.last_time - self.first_time
        print "EventActivity Span         " , self.first_occur_time, self.last_occur_time
        print "EventActivity Elapsed time ", self.last_occur_time - self.first_occur_time


        self.timehist.Histogram_timekey(self.bucket_minutes)

        print "\n====  # of DELETE_COMPLETE's per vm ordered by frequency"
        self.vm.Histogram_eventspans()

        print "\n==== HA id's by frequency"
        self.ha_id.Histogram(True)


    def IngestLine(self,line,log_time):
        # DELETE_COMPLETE for each HA id'
        #  HA_ id  - field 8
        #  vm name - field 15

        # Remember total time span for input stream
        # Handle DateTimes going backwards on a new log start in the input stream.
        if self.first_time == 0:
            self.first_time = log_time
            self.last_time = log_time
        else:
            self.first_time = min(self.first_time, log_time)
            self.last_time  = max(self.last_time , log_time)


        # Look for a DELETE_COMPLETE
        finds = self.re_del.findall(line)
        if len(finds) == 0:
            return

        # Found a DELETE_COMPLETE

        # Track span of deletes across all logs
        if self.first_occur_time == 0:
            self.first_occur_time = log_time
            self.last_occur_time = log_time
        else:
            self.first_occur_time = min(self.first_occur_time, log_time)
            self.last_occur_time = max(self.last_occur_time,log_time)



        # Parse line for relevant id's and place into
        # one or more reporting objects: histograms, etc.
        fields = line.split()
        if len(fields) < max(self.del_haid_fld, self.del_vm_fld):
            # print "Warning: Non standard log Line with DELETE_COMPLETE. Not including in data. "
            # print line

            #Ignoring these "stackstate" log messages.

            #2019-06-05 11:09:40,598 DEBUG
            #[com.ericsson.oss.itpf.enmdeploymentworkflows.heat.MonitorStackDeletion]
            #(job-executor-tp-threads - 984)
            #executeTask:stackType=sdncontroller, url=<URL
            #removed>,#stackState=DELETE_COMPLETE
            return

        id = fields[self.del_haid_fld]
        id = id[3:]  # strip off the leading HA_
        vmname = fields[self.del_vm_fld]

        # Track HA Event Occurrences by vm and HA iD

        self.ha_id.AddEvent(id,vmname,log_time)
        self.vm.AddEvent(vmname,id,log_time)

        key = bucket(log_time,self.bucket_minutes)
        #FIXME: keep key as datestamp so span can be done in Histogram_timekey()
        # Still need to alter time though
        self.timehist.AddTimeEvent(key,vmname,log_time)

        return



# Feed stdin to all specified processing modules.
# Call Report() method on modules after the log is consumed.
def main():

    # You can write new classes with IngestLine() and Report()
    # methods, and insert them there
    modules = []
    modules.append(EventActivityFinder())

    # Pass files to use as command line arguments or pipe in on stdin.
    for line in fileinput.input(openhook=fileinput.hook_compressed):
        try:
            lt = logTime(line)  # avoid reparsing time in all modules
        except ValueError:
            # Not a valid date stamped log line.  Stack dumps, empty lines, etc.
            continue

        for m in modules:
            m.IngestLine(line,lt)

    for m in modules:
        m.Report()

# TODO: Try to detect "unhealthy" HA workflows Difficult without
#  knowing the possible logfile outputs for all the HA workflow states
#  and transitions.  Need access to ENM source code, and possibly
#  adding some log messages for unlogged states.


if __name__ == "__main__":
    main()
