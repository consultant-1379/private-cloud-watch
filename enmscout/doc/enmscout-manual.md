# enmscout: The ENM Scout

Written by Loren Jan Wilson and Dan Barrett, 2019

## What is the purpose of ENM Scout?

We have some number of ENM deployments running in the same cloud, which
presents a few basic challenges:

1. Real-time visibility into the health of the application is low.
2. We don't have a single place to look to see what's happening across the
   whole cluster.

The **enm-viz** ENM visualizer is designed to help with these problems, but its
focus is on visualization, and it doesn't provide text reports that could be
sent to customers or further analyzed using other tools. It also uses Consul
data exclusively as its data source.

In order to further assist troubleshooting efforts, **enmscout** provides
multiple text-based views, and much like enm-viz, it does so in near real-time.
Some of the data generated by enmscout is designed to be used as data inputs by
other tools, so it's meant to be kind of a "middle point" between the
deployments and the investigators.

ENM Scout uses data sourced from the VNF-LAF jBoss logs, specifically the HA
workflow events, as well as the Consul state data. This can serve as a useful
alternate data source to what's available via Consul cluster monitoring, and
can be especially helpful during periods of intermittent platform degradation.

## What does ENM Scout provide?

ENM Scout currently provides the following reports, one per month, served over
the web:

- An events log generated from the HA workflows.
  - One event per VM recovery attempt, with detailed information about each.
  - Time-synchronized to UTC across all deployments.
  - Updated every 10 minutes.
  - Output available in two formats:
    - CSV (for importing into Excel)
    - JSON (for using as input to Perl and Python scripts)
- An events log showing VM Consul status.
  - Generated from data provided by the enm-viz monitoring Prometheus agents.
  - Updated every 2 minutes.
  - Output also available in CSV and JSON, as above.
- An HA workflow analysis report per month.
  - Shows several useful histograms which can be used to analyze HA workflow
    activity and performance.
  - Time-synchronized to UTC across all deployments.
  - Updated every 10 minutes.

Also, logging into the node that runs ENM Scout will give the user access to
full UTC time-synced logs for all production tenants, which can be used for
further analysis, as well a command line utility called `ha-state.py` which can
be run by hand to analyze and provide insight into any subset of those jBoss
logs.

## What's an "HA event"?

When instability is detected in an ENM deployment, which can happen for a
variety of reasons, the VNF-LAF lifecycle management engine tries to restore
stability by deleting and recreating the VMs that host ENM's application
components. From a high level, the operation looks like this:

1. One or more VMs fail some of their health checks that run on the node level,
   or the VM itself goes into "status left" in the Consul cluster.
2. This is noticed by a component called the SAM (simple availability manager),
   which raises a monitoring alert for the VMs.
3. The VNF-LAF picks up the monitoring alert and triggers a workflow called
   HighAvailabilityWorkflow, or HA workflow, for this VM or group of VMs.
4. The HA workflow goes through a series of steps in an attempt to recover the
   VMs in question, then checks to see if the VMs have returned to Consul.

When ENM Scout logs an HA event, what this means is that it looked in the VNF
logs and noticed that an HA workflow fired, and as a result, it creates one "HA
event" per VM that VNF-LAF is attempting to recover. This is an important
detail: one HA workflow run can trigger multiple HA events to be logged, one
per VM.

## What causes the HA workflow to fire?

Anything that causes a node to fail one of its own health checks or drop out of
Consul for any length of time (even a few seconds) can trigger an HA workflow.
Surrounding circumstances can include, but are certainly not limited to:

- Underlying OpenStack platform issues:
  - Certain types of network connectivity problems:
    - on the network used for VM to VM communication
    - on the network used to access network-based central storage
  - Central storage slowness or timeouts:
    - storage rebalancing
    - hard drive failure
    - raid controller failure
- VM application issues
- VM infrastructure issues:
  - full virtual disk partitions
  - high CPU utilization
  - running out of memory due to memory leaks

## What are the limitations of the HA workflow?

Interesting situations which will not trigger HA workflow events are:

- Network connectivity issues on the path from ENM to the customer.
- Central NFS issues inside a deployment.

There are likely other cases we haven't yet knowingly experienced which affect
application functionality or performance but aren't caught by the node-level
health checks and/or do not cause a VM to leave the Consul cluster.

Thus, it's important to note that HA events, even when added to the Consul
events, do not tell the whole story about application availability and
performance. A lot more could be learned by implementing real-time monitoring
for ENM that focuses on core application functionality and performance metrics.

## What are the steps taken by an HA workflow?

As of October 2019, HA workflow steps are roughly as follows:

1. HA workflow start
2. Look up VM by name using OpenStack Nova API and pull all info about it
3. Pull a list of stack names to stack urls
4. Use something from that query (IP?) to identify the corresponding Heat stack
5. Acquire lock on some part of the Heat stack (outer stack?)
6. Make a Nova request which goes to the compute node to stop the VM if it's running
7. Send a delete request to Heat for the VM, in some cases
8. Mark some part of the Heat stack unhealthy... either the inner stack, or the VM resource
9. Send an update request to the Heat stack, causing components marked unhealthy to be recreated
10. Wait for VMs to come back up and register in Consul
11. If a VM comes back: status “cleared” for the FM alarm for the VM
12. If the HA workflow waits for 5.5 minutes and a VM didn’t come back,
"updating severity to critical of FM alarm", then "retrying HA workflow for
VMs" and the process restarts.

IMPORTANT NOTE: there is NO log message when a workflow ends, so the "recovered
time" in an HA event is the time of the "successfully restored" message seen in
the jBoss logs.

## What does an HA event look like?

One HA event in JSON format looks like this:
```
{
  "deployment": "sprint",
  "vm": "bmasenm01-medrouter-1",
  "ha_id": "1569998875591",
  "start_time": "2019-10-02T06:47:55.601",
  "recovered_time": "2019-10-02T06:50:45.747",
  "attempts": 1
}
```

Each HA event contains the following attributes:

1. The name of the deployment, also known as the customer name or tenant name.
These names are gleaned from the "deployments" configuration file, from a
mapping between the VNF-LAF services node IP and this name. The customer name
does not actually show up anywhere in the VNF-LAF logs.
2. The name of the VM.
3. The HA workflow ID, which can be helpful when looking for this HA workflow
event in the logs. Many, but not all, of the log lines for this particular
workflow execution contain this ID.
4. The start time of this HA workflow, in UTC.
5. The recovered or end time of this HA workflow, in UTC. This value might be
null in cases where the workflow is either currently in progress and hasn't
finished recovering the VM yet, or where the workflow gave up on recovery the
VM for any reason.
6. A number of attempts to recover this particular VM made by this particular
workflow, which are calculated by counting the number of times that
resources are marked unhealthy in the OpenStack Heat stacks by the workflow.

## What's a "Consul event"?

Each ENM deployment uses Consul to cluster the VMs together, which serves at
least two purposes that we know of:

1. Consul is used as a service registry, which VMs use to figure out how to
   contact other VMs in its deployment.
2. Consul is also used by the Simple Availability Manager and the VNF-LAF to
   monitor VM presence in the cluster, as sort of a shorthand for whether or
   not the VM should be considered "up".

Each VM has a "status" in Consul, which is actually its Serf member status.
The status can be one of the following, as seen in the Serf source code:

```
const (
    StatusNone MemberStatus = iota
    StatusAlive
    StatusLeaving
    StatusLeft
    StatusFailed
)
```

The consulProm agent which is a part of enm-viz collects and presents this data
on a per-deployment basis, collecting it by SSHing into the services nodes.
Each time the consul-event-generator runs, it connects to the consulProm agents
and collects this data and stores it in a state file. Whenever a VM changes
status from the last time it was seen, consul-event-generator writes a Consul
event to its logs.

## What does a Consul event look like?

A Consul event in JSON format looks like this:
```
{
  "event_time": "2019-10-02T06:47:55.601"
  "deployment": "sprint",
  "vm": "bmasenm01-medrouter-1",
  "message": "transition from status Alive to Left"
}
```

Each event contains the following attributes:

1. The time this event was noticed. Note: this is the run time of the
consul-events-generator and not necessarily the time that this change
actually occurred.
2. The name of the deployment, gleaned from the consulProm configuration file.
3. The VM name.
4. A message describing the event.

## What are the limitations of Consul events?

The Consul events can only tell you that a particular VM changed state in
Consul. There are a large number of application availability and performance
issues that will not necessarily cause VMs to leave Consul, and conversely,
there are circumstances that can cause a node to leave Consul (such as Consul
application issues) even though the application is doing fine.

At this time, the events generator only reports on state changes from moment to
moment; it isn't qualified to make a judgment of whether a particular event
means that something is broken. In order to figure out the meaning and
importance of the events, a human will need to read through and judge them.

It would be possible to use these events as input for something like an
alerting or reporting tool, which could attempt to make judgments for each
event using context (grouping events together) and/or rule-based policy.

## What are the components of ENM Scout?

ENM Scout is made of 6 components, written in Python:

- `vnflaf-log-slurper.py`
  - Downloads and synchronizes jBoss logs from all production VNF-LAF services
    nodes, writing them out to disk in per-deployment subdirectories.
- `vnflaf-events-generator.py`
  - Parses through jBoss logs to generate events for each HA recovery attempt,
    one event per VM. Outputs in CSV and JSON.
- `ha-state.py`
  - Analyzes a given list of jBoss logs and provides useful histograms showing
    HA workflow events and retries for each VM, which can be used for fast and
    in-depth troubleshooting.
- `vnflaf-reporter.py`
  - Runs the log slurper and then the events-generator and ha-state reports, in
    the right order. Meant to be run periodically from cron.
- `consul-events-generator.py`
  - Generates VM state events from data provided by the enm-viz monitoring
    Prometheus agents.
- `enmscout.py`
  - Provides common functions used in the rest of the code.

## How can ENM Scout be installed?

The installation is very flexible; the software suite is configured using a
global configuration file. The test environment is installed this way:

- Copy the python code to `/opt/enmscout/bin`

- Copy the configuration files to `/opt/enmscout/etc`

- Check the configuration files to make sure that they make sense for your
  environment.

- Make a system account for enmscout.
```
sudo adduser --system enmscout --group
```

- Make directories that enmscout will use, and chown them.
```
for dir in /run/lock/enmscout /var/log/vnflaf /var/log/enmscout /var/tmp/enmscout /var/www/html
do
    sudo mkdir -p "${dir}"
    sudo chown -R enmscout:enmscout "${dir}"
done
```

- Acquire the SSH keys that will be required to contact each deployment (stored
in various locations on the genie-utility VM), and put them in
`/opt/enmscout/etc/keys`

- Make sure the enmscout user is the only one who can read that directory, too
```
sudo chown -R enmscout:enmscout /opt/enmscout/etc/keys
sudo chmod 700 /opt/enmscout/etc/keys
```

- Install a crontab for the enmscout user that looks like this:
```
*/10 * * * * /opt/enmscout/bin/vnflaf-reporter.py >/dev/null
*/2 * * * * /opt/enmscout/bin/consul-events-generator.py >/dev/null
```

- Tune the timing of those enmscout cronjobs based on your needs and the speed
  of the executions.

## What are the current known caveats?

- In order to contact the deployments, this code uses the SSH keys, rather than a
password. If those keys change, someone will need to copy the new ones for this
tool to use. This is not ideal, and should be automated.

- The software currently has to be installed manually. It could use an installer.

- Certain components of this software, especially the HA log copying and UTC
time synchronization, might require further optimization in order to scale
up further. One example: It may make more sense to change the VNF-LAF nodes so
that they log continuously to a central log server using syslog, and mark them
with the UTC time as they arrive.

- It is not yet known what connectivity will be like between datacenters, and
as such, it's not certain whether it will be necessary to run multiple
instances of this app, one per datacenter, or whether it will be possible or
optimal to run one central instance that collects data from all datacenters.


## Where can I go for more information?

- There is another document showing how to use `ha-state.py` in great detail.
- There are a lot of code comments strewn throughout the code which can be used
to figure out what's going on and why.
- There are log files written to disk by each component which can also help
figure out what's going on, especially if things don't seem to be working
properly.
